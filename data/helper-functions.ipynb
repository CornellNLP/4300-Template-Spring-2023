{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4340a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import re \n",
    "import matplotlib.pyplot as plt \n",
    "import statistics \n",
    "from nltk.tokenize import TreebankWordTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09ed311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "df = pd.read_csv('./archive/simplified_coffee.csv')\n",
    "#placeholder until we get database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8800547",
   "metadata": {},
   "outputs": [],
   "source": [
    "#database is array of reviews, array of names\n",
    "name_arr = list() #place holder\n",
    "review_arr = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5979651",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hard code query for sake of demo\n",
    "query = 'citrus chocolate bean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eaeb3537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Returns a list of words that make up the text.\n",
    "    Params: {text: String}\n",
    "    Returns: List\n",
    "    \"\"\"\n",
    "    return re.findall('[a-z]+', text.lower())\n",
    "\n",
    "def tokenize_reviews(df):\n",
    "    '''\n",
    "    Returns a dictionary with all reviews and their tokenized words\n",
    "    '''\n",
    "    tokens = set()\n",
    "    review_dict = dict()\n",
    "    for review in df:        \n",
    "        review_dict[df['name'][1]] = tokenize(df['review'][1])\n",
    "    return review_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8fc9f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_dict = tokenize_reviews(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca8db646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(x,y):\n",
    "    num =set(x).intersection(set(y))\n",
    "    denom = len(x) + len(y) - len(num)\n",
    "    return float(len(num) / denom)\n",
    "\n",
    "def build_cbeans_sims_jac(n_cbeans, input_query_cats, input_data):\n",
    "    \"\"\"Returns a cbeans_sims_jac matrix of size (num_cbeans,num_cbeans) where for (i,j) :\n",
    "        [i,j] should be the jaccard similarity between the category sets for cbeans i and j\n",
    "        such that cbeans_sims_jac[i,j] = cbeans_sims_jac[j,i]. \n",
    "        \n",
    "    \n",
    "    Params: {n_bean: Integer, the number of coffeebeans,\n",
    "            input_data: List<Dictionary>, a list of dictionaries where each dictionary \n",
    "                     represents the review_data including the script and the metadata of each movie script}\n",
    "            input_query_cats: user's input query categories\n",
    "    Returns: Numpy Array \n",
    "    \"\"\"\n",
    "    cbeans_sims_jac = np.ones((n_cbeans))\n",
    "    for cbean_idx in range(n_mov):\n",
    "        cat1 = input_data[cbean_idx]['categories']\n",
    "        jac = jaccard(cat1, input_query_cats)\n",
    "        cbeans_sims_jac[movie1_idx, movie2_idx] = jac\n",
    "                \n",
    "    return cbeans_sims_jac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9166154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosim\n",
    "#assume reviews is a dict with bean: tokenized review\n",
    "def build_inverted_index(review_dicts):\n",
    "    inverted_index = dict() #dictionary with word: list of tuples\n",
    "    doc_id = 0\n",
    "    for bean, review in review_dicts.keys(): #go thru each dict \n",
    "        #create a temp dict for count of words in tokenized_dict\n",
    "        temp_dict = {}\n",
    "        for token in review:\n",
    "            temp_dict[token] = temp_dict.get(token, 0) + 1 #get count of each token\n",
    "        \n",
    "        #go thru every word in temp_dict\n",
    "        for word, count in temp_dict.items():\n",
    "            if word in inverted_index:\n",
    "                inverted_index[word].append( (doc_id, count))\n",
    "            else: \n",
    "                inverted_index[word] = list() #initialize as list first idk if necessary\n",
    "                inverted_index[word].append((doc_id, count))\n",
    "        #move onto next doc\n",
    "        doc_id += 1 \n",
    "        \n",
    "        #now add counts to overall dictionary \n",
    "\n",
    "    return inverted_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2e41a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(inv_idx, n_docs, min_df=10, max_df_ratio=0.95):\n",
    "    \"\"\" Compute term IDF values from the inverted index.\"\"\"\n",
    "    \n",
    "    idf_vals = dict()\n",
    "    max_thresh = max_df_ratio * n_docs\n",
    "    for term, docs in inv_idx.items():\n",
    "        #print(type(docs))\n",
    "        len_docs = len(docs)\n",
    "        if len_docs<=max_thresh and len_docs>=10:\n",
    "            pre_log_idf = (n_docs/(1+len_docs))\n",
    "            idf = math.log2(pre_log_idf)\n",
    "            idf_vals[term] = idf\n",
    "    return idf_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97c72c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_doc_norms(index, idf, n_docs):\n",
    "    \"\"\" Precompute the euclidean norm of each document.\n",
    "    \n",
    "    norms: np.array, size: n_docs\n",
    "        norms[i] = the norm of document i.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    norms = np.zeros(n_docs)\n",
    "    for word in index:\n",
    "        \n",
    "        if word in idf:\n",
    "            idf_weight = idf[word]\n",
    "        else:\n",
    "            idf_weight = 0 #prune to 0\n",
    "        for doc in index[word]:\n",
    "            tf_weight = doc[1]\n",
    "            doc_id = doc[0]\n",
    "            norms[doc_id] += (tf_weight * idf_weight) ** 2\n",
    "    norms = np.sqrt(norms)\n",
    "     #go thru all possible docs, find the word and its invertex index, \n",
    "     #keep sum of product of tf number of times the word i appears in document j * idf[word]\n",
    "    return norms \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19c105b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_dot_scores(query_word_counts, index, idf):\n",
    "    \"\"\" Perform a term-at-a-time iteration to efficiently compute the numerator term of cosine similarity across multiple documents.\n",
    "   \n",
    "    Returns\n",
    "    =======\n",
    "    \n",
    "    doc_scores: dict\n",
    "        Dictionary mapping from doc ID to the final accumulated score for that doc\n",
    "    \"\"\"\n",
    "\n",
    "    doc_scores = dict() \n",
    "    \n",
    "    for word, qf in query_word_counts.items(): \n",
    "        if word in index:\n",
    "            documents = index[word]\n",
    "            for doc in documents: \n",
    "                doc_id, tf = doc[0], doc[1]\n",
    "\n",
    "                if word not in idf: \n",
    "                    idf_val = 0\n",
    "                else:\n",
    "                    idf_val = idf[word]\n",
    "\n",
    "                acc = idf_val * qf * tf * idf_val\n",
    "                if doc_id not in doc_scores:\n",
    "                    doc_scores[doc_id] = acc\n",
    "                else:\n",
    "                    doc_scores[doc_id] = doc_scores[doc_id] + acc\n",
    "    return doc_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "660ab745",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inv_idx \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_inverted_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview_dict\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#TO DO : CHANGE IT TO A LIST\u001b[39;00m\n\u001b[1;32m      3\u001b[0m idf \u001b[38;5;241m=\u001b[39m compute_idf(inv_idx, \u001b[38;5;28mlen\u001b[39m(review_dict),\n\u001b[1;32m      4\u001b[0m                   min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m      5\u001b[0m                   max_df_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)  \u001b[38;5;66;03m# documents are very short so we can use a small value here\u001b[39;00m\n\u001b[1;32m      6\u001b[0m                                      \u001b[38;5;66;03m# examine the actual DF values of common words like \"the\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m                                      \u001b[38;5;66;03m# to set these values\u001b[39;00m\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mbuild_inverted_index\u001b[0;34m(reviews)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tokenized \u001b[38;5;129;01min\u001b[39;00m reviews: \u001b[38;5;66;03m#go thru each dict \u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#create a temp dict for count of words in tokenized_dict\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     temp_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokenized\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtoks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     10\u001b[0m         temp_dict[token] \u001b[38;5;241m=\u001b[39m temp_dict\u001b[38;5;241m.\u001b[39mget(token, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m#get count of each token\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#go thru every word in temp_dict\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "inv_idx = build_inverted_index(review_dict) #TO DO : CHANGE IT TO A LIST\n",
    "\n",
    "idf = compute_idf(inv_idx, len(review_dict),\n",
    "                  min_df=10,\n",
    "                  max_df_ratio=0.1)  # documents are very short so we can use a small value here\n",
    "                                     # examine the actual DF values of common words like \"the\"\n",
    "                                     # to set these values\n",
    "\n",
    "inv_idx = {key: val for key, val in inv_idx.items()\n",
    "           if key in idf}            # prune the terms left out by idf\n",
    "\n",
    "doc_norms = compute_doc_norms(inv_idx, idf, len(review_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e04400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_search(query, index, idf, doc_norms, score_func=accumulate_dot_scores, tokenizer=TreebankWordTokenizer):\n",
    "    \"\"\" Search the collection of documents for the given query\n",
    "   \n",
    "    Returns\n",
    "    =======\n",
    "    \n",
    "    results, list of tuples (score, doc_id)\n",
    "        Sorted list of results such that the first element has\n",
    "        the highest score, and `doc_id` points to the document\n",
    "        with the highest score.\n",
    "\n",
    "    \"\"\"\n",
    "    query = query.lower() \n",
    "    query_tokens = tokenizer.tokenize(query)\n",
    "    query_word_counts = dict()\n",
    "    \n",
    "\n",
    "    for word in query_tokens:\n",
    "        query_word_counts[word] = query_word_counts.get(word, 0) + 1 \n",
    "    results = list() \n",
    "    doc_scores = score_func(query_word_counts, index, idf)\n",
    "    #q_norms\n",
    "    q_norm = 0 \n",
    "    for term, freq in query_word_counts.items():\n",
    "        if term in idf:\n",
    "            idf_weight = idf[term]\n",
    "        else:\n",
    "            idf_weight = 0 #prune to 0\n",
    "        q_norm += ((freq  * idf_weight)  ** 2)\n",
    "    q_norm = math.sqrt(q_norm)\n",
    "    \n",
    "    for doc_id, doc_score in doc_scores.items():\n",
    "        cossim_val = doc_score / (doc_norms[doc_id] *  q_norm)\n",
    "        results.append((cossim_val,doc_id))\n",
    "        \n",
    "    results = sorted(results, key=lambda x: x[0], reverse=True)\n",
    "    return results[0:10] #return first top ten similar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d03c150e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inv_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m index_search(query, \u001b[43minv_idx\u001b[49m, idf, doc_norms)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inv_idx' is not defined"
     ]
    }
   ],
   "source": [
    "output_scores, output_ids = index_search(query, inv_idx, idf, doc_norms) #score, doc id \n",
    "rel_beans = name_arry[output_ids]\n",
    "rel_beans_revs = rev_array[output_ids]\n",
    "#rel_beans should be top 10 most similar cbeans & reviews & what frontend displays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f11623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
