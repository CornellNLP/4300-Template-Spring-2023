{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4340a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import re \n",
    "import matplotlib.pyplot as plt \n",
    "import statistics \n",
    "from nltk.tokenize import TreebankWordTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09ed311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "df = pd.read_csv('./archive/simplified_coffee.csv')\n",
    "#placeholder until we get database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eaeb3537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Returns a list of words that make up the text.\n",
    "    \n",
    "    Note: for simplicity, lowercase everything.\n",
    "    Requirement: Use Regex to satisfy this function\n",
    "    \n",
    "    Params: {text: String}\n",
    "    Returns: List\n",
    "    \"\"\"\n",
    "    return re.findall('[a-z]+', text.lower())\n",
    "\n",
    "def tokenize_reviews(df):\n",
    "    '''\n",
    "    Returns a dictionary with all reviews and their tokenized words\n",
    "    '''\n",
    "    tokens = set()\n",
    "    review_dict = dict()\n",
    "    for review in df:        \n",
    "        review_dict[df['name'][1]] = tokenize(df['review'][1])\n",
    "    return review_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8fc9f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_dict = tokenize_reviews(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca8db646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(x,y):\n",
    "    num =set(x).intersection(set(y))\n",
    "    denom = len(x) + len(y) - len(num)\n",
    "    return float(len(num) / denom)\n",
    "\n",
    "def build_cbeans_sims_jac(n_cbeans, input_query_cats, input_data):\n",
    "    \"\"\"Returns a cbeans_sims_jac matrix of size (num_cbeans,num_cbeans) where for (i,j) :\n",
    "        [i,j] should be the jaccard similarity between the category sets for cbeans i and j\n",
    "        such that cbeans_sims_jac[i,j] = cbeans_sims_jac[j,i]. \n",
    "        \n",
    "    Note: \n",
    "        coffeebeans sometimes contain *duplicate* categories! You should only count a category once\n",
    "        \n",
    "        A coffeebeans should have a jaccard similarity of 1.0 with itself.\n",
    "    \n",
    "    Params: {n_bean: Integer, the number of coffeebeans,\n",
    "            input_data: List<Dictionary>, a list of dictionaries where each dictionary \n",
    "                     represents the review_data including the script and the metadata of each movie script}\n",
    "            input_query_cats: user's input query categories\n",
    "    Returns: Numpy Array \n",
    "    \"\"\"\n",
    "    cbeans_sims_jac = np.ones((n_cbeans))\n",
    "    for cbean_idx in range(n_mov):\n",
    "        cat1 = input_data[cbean_idx]['categories']\n",
    "        jac = jaccard(cat1, input_query_cats)\n",
    "        cbeans_sims_jac[movie1_idx, movie2_idx] = jac\n",
    "                \n",
    "    return cbeans_sims_jac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9166154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosim\n",
    "#assume reviews is a list of dicts that contain tokenized mesage\n",
    "def build_inverted_index(reviews):\n",
    "    inverted_index = dict() #dictionary with word: list of tuples\n",
    "    doc_id = 0\n",
    "    for tokenized in reviews: #go thru each dict \n",
    "        #create a temp dict for count of words in tokenized_dict\n",
    "        temp_dict = {}\n",
    "        for token in tokenized['toks']:\n",
    "            temp_dict[token] = temp_dict.get(token, 0) + 1 #get count of each token\n",
    "        \n",
    "        #go thru every word in temp_dict\n",
    "        for word, count in temp_dict.items():\n",
    "            if word in inverted_index:\n",
    "                inverted_index[word].append( (doc_id, count))\n",
    "            else: \n",
    "                inverted_index[word] = list() #initialize as list first idk if necessary\n",
    "                inverted_index[word].append((doc_id, count))\n",
    "        #move onto next doc\n",
    "        doc_id += 1 \n",
    "        \n",
    "        #now add counts to overall dictionary \n",
    "\n",
    "    return inverted_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2e41a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(inv_idx, n_docs, min_df=10, max_df_ratio=0.95):\n",
    "    \"\"\" Compute term IDF values from the inverted index.\n",
    "    Words that are too frequent or too infrequent get pruned.\n",
    "    \n",
    "    Hint: Make sure to use log base 2.\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "    \n",
    "    inv_idx: an inverted index as above\n",
    "    dict of words: # docs they appear in\n",
    "    \n",
    "    n_docs: int,\n",
    "        The number of documents.\n",
    "        \n",
    "    min_df: int,\n",
    "        Minimum number of documents a term must occur in.\n",
    "        Less frequent words get ignored. \n",
    "        Documents that appear min_df number of times should be included.\n",
    "    \n",
    "    max_df_ratio: float,\n",
    "        Maximum ratio of documents a term can occur in.\n",
    "        More frequent words get ignored.\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    \n",
    "    idf: dict\n",
    "        For each term, the dict contains the idf value.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    idf_vals = dict()\n",
    "    max_thresh = max_df_ratio * n_docs\n",
    "    for term, docs in inv_idx.items():\n",
    "        #print(type(docs))\n",
    "        len_docs = len(docs)\n",
    "        if len_docs<=max_thresh and len_docs>=10:\n",
    "            pre_log_idf = (n_docs/(1+len_docs))\n",
    "            idf = math.log2(pre_log_idf)\n",
    "            idf_vals[term] = idf\n",
    "    return idf_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97c72c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_doc_norms(index, idf, n_docs):\n",
    "    \"\"\" Precompute the euclidean norm of each document.\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "    \n",
    "    index: the inverted index as above\n",
    "    \n",
    "    idf: dict,\n",
    "        Precomputed idf values for the terms.\n",
    "    \n",
    "    n_docs: int,\n",
    "        The total number of documents.\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    \n",
    "    norms: np.array, size: n_docs\n",
    "        norms[i] = the norm of document i.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    norms = np.zeros(n_docs)\n",
    "    for word in index:\n",
    "        \n",
    "        if word in idf:\n",
    "            idf_weight = idf[word]\n",
    "        else:\n",
    "            idf_weight = 0 #prune to 0\n",
    "        for doc in index[word]:\n",
    "            tf_weight = doc[1]\n",
    "            doc_id = doc[0]\n",
    "            norms[doc_id] += (tf_weight * idf_weight) ** 2\n",
    "    norms = np.sqrt(norms)\n",
    "     #go thru all possible docs, find the word and its invertex index, \n",
    "     #keep sum of product of tf number of times the word i appears in document j * idf[word]\n",
    "    return norms \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19c105b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_dot_scores(query_word_counts, index, idf):\n",
    "    \"\"\" Perform a term-at-a-time iteration to efficiently compute the numerator term of cosine similarity across multiple documents.\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "    \n",
    "    query_word_counts: dict,\n",
    "        A dictionary containing all words that appear in the query;\n",
    "        Each word is mapped to a count of how many times it appears in the query.\n",
    "        In other words, query_word_counts[w] = the term frequency of w in the query.\n",
    "        You may safely assume all words in the dict have been already lowercased.\n",
    "    \n",
    "    index: the inverted index as above,\n",
    "    \n",
    "    idf: dict,\n",
    "        Precomputed idf values for the terms.\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    \n",
    "    doc_scores: dict\n",
    "        Dictionary mapping from doc ID to the final accumulated score for that doc\n",
    "    \"\"\"\n",
    "\n",
    "    doc_scores = dict() \n",
    "    \n",
    "    for word, qf in query_word_counts.items(): \n",
    "        if word in index:\n",
    "            documents = index[word]\n",
    "            for doc in documents: \n",
    "                doc_id, tf = doc[0], doc[1]\n",
    "\n",
    "                if word not in idf: \n",
    "                    idf_val = 0\n",
    "                else:\n",
    "                    idf_val = idf[word]\n",
    "\n",
    "                acc = idf_val * qf * tf * idf_val\n",
    "                if doc_id not in doc_scores:\n",
    "                    doc_scores[doc_id] = acc\n",
    "                else:\n",
    "                    doc_scores[doc_id] = doc_scores[doc_id] + acc\n",
    "    return doc_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "660ab745",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'flat_msgs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inv_idx \u001b[38;5;241m=\u001b[39m build_inverted_index(\u001b[43mflat_msgs\u001b[49m)\n\u001b[1;32m      3\u001b[0m idf \u001b[38;5;241m=\u001b[39m compute_idf(inv_idx, \u001b[38;5;28mlen\u001b[39m(flat_msgs),\n\u001b[1;32m      4\u001b[0m                   min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m      5\u001b[0m                   max_df_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)  \u001b[38;5;66;03m# documents are very short so we can use a small value here\u001b[39;00m\n\u001b[1;32m      6\u001b[0m                                      \u001b[38;5;66;03m# examine the actual DF values of common words like \"the\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m                                      \u001b[38;5;66;03m# to set these values\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'flat_msgs' is not defined"
     ]
    }
   ],
   "source": [
    "inv_idx = build_inverted_index(flat_msgs) #TO DO : CHANGE IT TO A LIST\n",
    "\n",
    "idf = compute_idf(inv_idx, len(flat_msgs),\n",
    "                  min_df=10,\n",
    "                  max_df_ratio=0.1)  # documents are very short so we can use a small value here\n",
    "                                     # examine the actual DF values of common words like \"the\"\n",
    "                                     # to set these values\n",
    "\n",
    "inv_idx = {key: val for key, val in inv_idx.items()\n",
    "           if key in idf}            # prune the terms left out by idf\n",
    "\n",
    "doc_norms = compute_doc_norms(inv_idx, idf, len(flat_msgs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e04400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_search(query, index, idf, doc_norms, score_func=accumulate_dot_scores, tokenizer=TreebankWordTokenizer):\n",
    "    \"\"\" Search the collection of documents for the given query\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "    \n",
    "    query: string,\n",
    "        The query we are looking for.\n",
    "    \n",
    "    index: an inverted index as above\n",
    "    \n",
    "    idf: idf values precomputed as above\n",
    "    \n",
    "    doc_norms: document norms as computed above\n",
    "    \n",
    "    score_func: function,\n",
    "        A function that computes the numerator term of cosine similarity (the dot product) for all documents.\n",
    "        Takes as input a dictionary of query word counts, the inverted index, and precomputed idf values.\n",
    "        (See Q7)\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    \n",
    "    results, list of tuples (score, doc_id)\n",
    "        Sorted list of results such that the first element has\n",
    "        the highest score, and `doc_id` points to the document\n",
    "        with the highest score.\n",
    "    \n",
    "    Note: \n",
    "        \n",
    "    \"\"\"\n",
    "    query = query.lower() \n",
    "    query_tokens = tokenizer.tokenize(query)\n",
    "    query_word_counts = dict()\n",
    "    \n",
    "\n",
    "    for word in query_tokens:\n",
    "        query_word_counts[word] = query_word_counts.get(word, 0) + 1 \n",
    "    results = list() \n",
    "    doc_scores = score_func(query_word_counts, index, idf)\n",
    "    #q_norms\n",
    "    q_norm = 0 \n",
    "    for term, freq in query_word_counts.items():\n",
    "        if term in idf:\n",
    "            idf_weight = idf[term]\n",
    "        else:\n",
    "            idf_weight = 0 #prune to 0\n",
    "        q_norm += ((freq  * idf_weight)  ** 2)\n",
    "    q_norm = math.sqrt(q_norm)\n",
    "    \n",
    "    for doc_id, doc_score in doc_scores.items():\n",
    "        cossim_val = doc_score / (doc_norms[doc_id] *  q_norm)\n",
    "        results.append((cossim_val,doc_id))\n",
    "        \n",
    "    results = sorted(results, key=lambda x: x[0], reverse=True)\n",
    "    return results[0:10] #return first top ten similar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da922812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46a72ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d53cf92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e5d535",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
